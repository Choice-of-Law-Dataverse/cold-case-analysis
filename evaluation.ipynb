{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "290f099b-ccba-4510-a776-fa23270f1009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Human_score  G_Eval_Score  \\\n",
      "Column                                                                         \n",
      "Abstract                             Human_score      1.000000     -0.456690   \n",
      "                                     G_Eval_Score    -0.456690      1.000000   \n",
      "                                     BERT_Avg         0.104745      0.415969   \n",
      "Choice of law issue                  Human_score      1.000000      0.434890   \n",
      "                                     G_Eval_Score     0.434890      1.000000   \n",
      "                                     BERT_Avg         0.557163      0.818316   \n",
      "Court's position                     Human_score      1.000000     -0.285247   \n",
      "                                     G_Eval_Score    -0.285247      1.000000   \n",
      "                                     BERT_Avg         0.003887      0.687458   \n",
      "PIL provisions                       Human_score      1.000000     -0.037008   \n",
      "                                     G_Eval_Score    -0.037008      1.000000   \n",
      "                                     BERT_Avg         0.322712     -0.169369   \n",
      "Relevant facts / Summary of the case Human_score      1.000000     -0.411959   \n",
      "                                     G_Eval_Score    -0.411959      1.000000   \n",
      "                                     BERT_Avg         0.851012     -0.492206   \n",
      "\n",
      "                                                   BERT_Avg  \n",
      "Column                                                       \n",
      "Abstract                             Human_score   0.104745  \n",
      "                                     G_Eval_Score  0.415969  \n",
      "                                     BERT_Avg      1.000000  \n",
      "Choice of law issue                  Human_score   0.557163  \n",
      "                                     G_Eval_Score  0.818316  \n",
      "                                     BERT_Avg      1.000000  \n",
      "Court's position                     Human_score   0.003887  \n",
      "                                     G_Eval_Score  0.687458  \n",
      "                                     BERT_Avg      1.000000  \n",
      "PIL provisions                       Human_score   0.322712  \n",
      "                                     G_Eval_Score -0.169369  \n",
      "                                     BERT_Avg      1.000000  \n",
      "Relevant facts / Summary of the case Human_score   0.851012  \n",
      "                                     G_Eval_Score -0.492206  \n",
      "                                     BERT_Avg      1.000000  \n",
      "Merged dataset saved as merged_evaluation_data.csv\n",
      "Overall correlation matrix saved as evaluation_correlation_matrix.csv\n",
      "              Human_score  G_Eval_Score  BERT_Avg\n",
      "Human_score      1.000000     -0.018741  0.204084\n",
      "G_Eval_Score    -0.018741      1.000000  0.319662\n",
      "BERT_Avg         0.204084      0.319662  1.000000\n",
      "Category-wise correlation matrix saved as evaluation_correlation_per_category.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53718/2036362970.py:10: DtypeWarning: Columns (0,1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  \"human_eval\": pd.read_csv(human_eval_path),\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "human_eval_path = \"cold_case_analyzer/data/gpt_evaluation_ranking_human.csv\"\n",
    "geval_eval_path = \"cold_case_analyzer/data/evaluations/geval_evaluation_detailed_20250213_092412.csv\"\n",
    "bertopic_eval_path = \"cold_case_analyzer/data/evaluations/bertopic_evaluation_detailed_20250213_092231.csv\"\n",
    "\n",
    "# Load the data\n",
    "dataframes = {\n",
    "    \"human_eval\": pd.read_csv(human_eval_path),\n",
    "    \"geval_eval\": pd.read_csv(geval_eval_path),\n",
    "    \"bertopic_eval\": pd.read_csv(bertopic_eval_path),\n",
    "}\n",
    "\n",
    "# Rename 'Column metric' in human_eval to match others\n",
    "dataframes[\"human_eval\"].rename(columns={\"Column metric\": \"Column\"}, inplace=True)\n",
    "\n",
    "# Define a mapping to standardize column names in human_eval\n",
    "column_mapping = {\n",
    "    \"Abstract Accuracy\": \"Abstract\",\n",
    "    \"Abstract Conciseness\": \"Abstract\",\n",
    "    \"Relevant Facts Accuracy\": \"Relevant facts / Summary of the case\",\n",
    "    \"Relevant Facts Focus on PIL\": \"Relevant facts / Summary of the case\",\n",
    "    \"Relevant Facts Conciseness\": \"Relevant facts / Summary of the case\",\n",
    "    \"PIL Provisions Adherence to Format\": \"PIL provisions\",\n",
    "    \"PIL Provisions Accuracy\": \"PIL provisions\",\n",
    "    \"Choice of Law Issue Classification Accuracy\": \"Choice of law issue\",\n",
    "    \"Choice of Law Issue Correct Identification of CoLI\": \"Choice of law issue\",\n",
    "    \"Choice of Law Issue Precision of Phrasing\": \"Choice of law issue\",\n",
    "    \"Court's Position Does it Answer the CoLI\": \"Court's position\",\n",
    "    \"Court's Position Conciseness\": \"Court's position\",\n",
    "}\n",
    "\n",
    "dataframes[\"human_eval\"][\"Column\"] = dataframes[\"human_eval\"][\"Column\"].replace(column_mapping)\n",
    "\n",
    "# Filter only common IDs\n",
    "common_ids = set(dataframes[\"geval_eval\"][\"ID\"]) & set(dataframes[\"bertopic_eval\"][\"ID\"])\n",
    "human_filtered = dataframes[\"human_eval\"][dataframes[\"human_eval\"][\"ID\"].isin(common_ids)]\n",
    "\n",
    "# Merge datasets\n",
    "merged_df = human_filtered.merge(dataframes[\"geval_eval\"], on=[\"ID\", \"Column\"], suffixes=('_human', '_geval'))\n",
    "merged_df = merged_df.merge(dataframes[\"bertopic_eval\"], on=[\"ID\", \"Column\"])\n",
    "\n",
    "# Compute BERTScore average\n",
    "merged_df[\"BERT_Avg\"] = merged_df[[\"BERT_Precision\", \"BERT_Recall\", \"BERT_F1\"]].mean(axis=1)\n",
    "\n",
    "# Compute correlation overall\n",
    "correlation_matrix = merged_df[[\"Human_score\", \"G_Eval_Score\", \"BERT_Avg\"]].corr()\n",
    "correlation_matrix.to_csv(\"cold_case_analyzer/data/evaluations/evaluation_correlation_matrix.csv\")\n",
    "\n",
    "# Compute correlation per category\n",
    "category_correlation = merged_df.groupby(\"Column\")[[\"Human_score\", \"G_Eval_Score\", \"BERT_Avg\"]].corr()\n",
    "category_correlation.to_csv(\"cold_case_analyzer/data/evaluations/evaluation_correlation_per_category.csv\")\n",
    "\n",
    "# Save outputs\n",
    "merged_df.to_csv(\"cold_case_analyzer/data/evaluations/merged_evaluation_data.csv\", index=False)\n",
    "\n",
    "# Print results\n",
    "print(\"Merged dataset saved as merged_evaluation_data.csv\")\n",
    "print(\"Overall correlation matrix saved as evaluation_correlation_matrix.csv\")\n",
    "print(correlation_matrix)\n",
    "print(\"Category-wise correlation matrix saved as evaluation_correlation_per_category.csv\")\n",
    "print(category_correlation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
